{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"accelerator":"GPU","language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30476,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"https://s8.hostingkartinok.com/uploads/images/2018/08/308b49fcfbc619d629fe4604bceb67ac.jpg\" width=500, height=450>\n<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>","metadata":{"id":"Ot3c4fjZwC4T"}},{"cell_type":"markdown","source":"---","metadata":{"id":"P2JdzEXmwRU5"}},{"cell_type":"markdown","source":"# Задание 3\n\n## Классификация текстов\n\nВ этом задании вам предстоит попробовать несколько методов, используемых в задаче классификации, а также понять насколько хорошо модель понимает смысл слов и какие слова в примере влияют на результат.","metadata":{"id":"TItJgQMp5p4E"}},{"cell_type":"code","source":"!pip install torchtext==0.6.0","metadata":{"execution":{"iopub.status.busy":"2023-05-19T19:21:38.753447Z","iopub.execute_input":"2023-05-19T19:21:38.754377Z","iopub.status.idle":"2023-05-19T19:21:51.523236Z","shell.execute_reply.started":"2023-05-19T19:21:38.754341Z","shell.execute_reply":"2023-05-19T19:21:51.522141Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torchtext==0.6.0\n  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from torchtext==0.6.0) (1.16.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from torchtext==0.6.0) (0.1.98)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torchtext==0.6.0) (4.64.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from torchtext==0.6.0) (2.0.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchtext==0.6.0) (1.23.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchtext==0.6.0) (2.28.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext==0.6.0) (3.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext==0.6.0) (2.1.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext==0.6.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext==0.6.0) (2022.12.7)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->torchtext==0.6.0) (3.1.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->torchtext==0.6.0) (4.5.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->torchtext==0.6.0) (3.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->torchtext==0.6.0) (1.11.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->torchtext==0.6.0) (3.11.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->torchtext==0.6.0) (2.1.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\nInstalling collected packages: torchtext\n  Attempting uninstall: torchtext\n    Found existing installation: torchtext 0.15.1\n    Uninstalling torchtext-0.15.1:\n      Successfully uninstalled torchtext-0.15.1\nSuccessfully installed torchtext-0.6.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\n\nfrom torchtext import datasets\n\nfrom torchtext.data import Field, LabelField\nfrom torchtext.data import BucketIterator\n\nfrom torchtext.vocab import Vectors, GloVe\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport random\nfrom tqdm.autonotebook import tqdm","metadata":{"id":"1emhHMhniZMu","execution":{"iopub.status.busy":"2023-05-19T19:21:51.526554Z","iopub.execute_input":"2023-05-19T19:21:51.526913Z","iopub.status.idle":"2023-05-19T19:21:54.614699Z","shell.execute_reply.started":"2023-05-19T19:21:51.526887Z","shell.execute_reply":"2023-05-19T19:21:54.613744Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/554992104.py:16: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm\n","output_type":"stream"}]},{"cell_type":"markdown","source":"В этом задании мы будем использовать библиотеку torchtext. Она довольна проста в использовании и поможет нам сконцентрироваться на задаче, а не на написании Dataloader-а.","metadata":{"id":"XyOlPZA26Ppy"}},{"cell_type":"code","source":"TEXT = Field(sequential=True, lower=True, include_lengths=True)  # Поле текста\nLABEL = LabelField(dtype=torch.float)  # Поле метки","metadata":{"id":"-VuQ1E10_tX_","execution":{"iopub.status.busy":"2023-05-19T19:21:54.616128Z","iopub.execute_input":"2023-05-19T19:21:54.617067Z","iopub.status.idle":"2023-05-19T19:21:54.623918Z","shell.execute_reply.started":"2023-05-19T19:21:54.617033Z","shell.execute_reply":"2023-05-19T19:21:54.621129Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"SEED = 1234\n\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","metadata":{"id":"SBK4ipzS122j","execution":{"iopub.status.busy":"2023-05-19T19:21:54.626326Z","iopub.execute_input":"2023-05-19T19:21:54.626782Z","iopub.status.idle":"2023-05-19T19:21:54.657255Z","shell.execute_reply.started":"2023-05-19T19:21:54.626746Z","shell.execute_reply":"2023-05-19T19:21:54.656222Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Датасет на котором мы будем проводить эксперементы это комментарии к фильмам из сайта IMDB.","metadata":{"id":"nNWLG7mG6n2d"}},{"cell_type":"code","source":"train, test = datasets.IMDB.splits(TEXT, LABEL)  # загрузим датасет\ntrain, valid = train.split(random_state=random.seed(SEED))  # разобьем на части","metadata":{"id":"mRzUSWeAi6Xq","execution":{"iopub.status.busy":"2023-05-19T19:21:54.658707Z","iopub.execute_input":"2023-05-19T19:21:54.659101Z","iopub.status.idle":"2023-05-19T19:22:32.916236Z","shell.execute_reply.started":"2023-05-19T19:21:54.659069Z","shell.execute_reply":"2023-05-19T19:22:32.915297Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"downloading aclImdb_v1.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:08<00:00, 10.2MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"TEXT.build_vocab(train)\nLABEL.build_vocab(train)","metadata":{"id":"uQfIRhWPjURL","execution":{"iopub.status.busy":"2023-05-19T19:22:32.921374Z","iopub.execute_input":"2023-05-19T19:22:32.922545Z","iopub.status.idle":"2023-05-19T19:22:34.175253Z","shell.execute_reply.started":"2023-05-19T19:22:32.922509Z","shell.execute_reply":"2023-05-19T19:22:34.174273Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ntrain_iter, valid_iter, test_iter = BucketIterator.splits(\n    (train, valid, test), \n    batch_size = 64,\n    sort_within_batch = True,\n    device = device)","metadata":{"id":"bSoBkdcj4roR","execution":{"iopub.status.busy":"2023-05-19T10:44:32.520449Z","iopub.execute_input":"2023-05-19T10:44:32.520848Z","iopub.status.idle":"2023-05-19T10:44:32.547776Z","shell.execute_reply.started":"2023-05-19T10:44:32.520814Z","shell.execute_reply":"2023-05-19T10:44:32.546545Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## RNN\n\nДля начала попробуем использовать рекурентные нейронные сети. На семинаре вы познакомились с GRU, вы можете также попробовать LSTM. Можно использовать для классификации как hidden_state, так и output последнего токена.","metadata":{"id":"3_CRDES360wG"}},{"cell_type":"code","source":"class GRU(nn.Module):\n    def __init__(self, embed_size, hidden_size):\n        super().__init__()\n\n        self.embed_size = embed_size\n        self.hidden_size = hidden_size\n\n        self.w_rh = nn.Parameter(torch.rand(hidden_size, hidden_size))\n        self.b_rh = nn.Parameter(torch.rand((1, hidden_size)))\n        self.w_rx = nn.Parameter(torch.rand(embed_size, hidden_size))\n        self.b_rx = nn.Parameter(torch.rand(1, hidden_size))\n\n        self.w_zh = nn.Parameter(torch.rand(hidden_size, hidden_size))\n        self.b_zh = nn.Parameter(torch.rand((1, hidden_size)))\n        self.w_zx = nn.Parameter(torch.rand(embed_size, hidden_size))\n        self.b_zx = nn.Parameter(torch.rand(1, hidden_size))\n\n        self.w_nh = nn.Parameter(torch.rand(hidden_size, hidden_size))\n        self.b_nh = nn.Parameter(torch.rand((1, hidden_size)))\n        self.w_nx = nn.Parameter(torch.rand(embed_size, hidden_size))\n        self.b_nx = nn.Parameter(torch.rand(1, hidden_size))\n\n    def forward(self, x, hidden = None):\n        '''\n        x – torch.FloatTensor with the shape (bs, seq_length, emb_size)\n        hidden - torch.FloatTensro with the shape (bs, hidden_size)\n        return: torch.FloatTensor with the shape (bs, hidden_size)\n        '''\n        if hidden is None:\n            hidden = torch.zeros((x.size(0), self.hidden_size)).to(x.device)\n        \n        for cur_idx in range(x.size(1)):\n            r = torch.sigmoid(\n                x[:, cur_idx] @ self.w_rx + self.b_rx + hidden @ self.w_rh + self.b_rh\n            )\n            z = torch.sigmoid(\n                x[:, cur_idx] @ self.w_zx + self.b_zx + hidden @ self.w_zh + self.b_zh\n            )\n            n = torch.tanh(\n                x[:, cur_idx] @ self.w_nx + self.b_nx + r * (hidden @ self.w_nh + self.b_nh)\n            )\n            hidden = (1 - z) * n + z * hidden\n\n        return hidden","metadata":{"execution":{"iopub.status.busy":"2023-05-19T10:44:32.552133Z","iopub.execute_input":"2023-05-19T10:44:32.553405Z","iopub.status.idle":"2023-05-19T10:44:32.570630Z","shell.execute_reply.started":"2023-05-19T10:44:32.553370Z","shell.execute_reply":"2023-05-19T10:44:32.569433Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class RNNBaseline(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n                 bidirectional, dropout, pad_idx):\n        \n        super().__init__()\n        \n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n        \n        self.rnn = GRU(embedding_dim, hidden_dim)\n        \n        self.fc = nn.Linear(hidden_dim, 2)\n        \n        self.softmax = nn.Softmax(dim=1)\n        \n        \n    def forward(self, text):\n        \n        #text = [sent len, batch size]\n        \n#         embedded = self.embedding(text)\n        \n#         #embedded = [sent len, batch size, emb dim]\n        \n#         #pack sequence\n#         packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n        \n        # cell arg for LSTM, remove for GRU\n#         packed_output, (hidden, cell) = self.rnn(packed_embedded)\n        #unpack sequence\n#         output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)  \n\n        #output = [sent len, batch size, hid dim * num directions]\n        #output over padding tokens are zero tensors\n        \n        #hidden = [num layers * num directions, batch size, hid dim]\n        #cell = [num layers * num directions, batch size, hid dim]\n        \n        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n        #and apply dropout\n        \n        embedded = self.embedding(text)\n        hidden = self.rnn(embedded)\n        output = self.softmax(self.fc(hidden))\n                \n        #hidden = [batch size, hid dim * num directions] or [batch_size, hid dim * num directions]\n            \n        return output","metadata":{"id":"J1yE1KPQqDat","execution":{"iopub.status.busy":"2023-05-19T10:44:32.574325Z","iopub.execute_input":"2023-05-19T10:44:32.574731Z","iopub.status.idle":"2023-05-19T10:44:32.589330Z","shell.execute_reply.started":"2023-05-19T10:44:32.574704Z","shell.execute_reply":"2023-05-19T10:44:32.588532Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Поиграйтесь с гиперпараметрами","metadata":{"id":"U7dLGFg4M7Te"}},{"cell_type":"code","source":"vocab_size = len(TEXT.vocab)\nemb_dim = 100\nhidden_dim = 256\noutput_dim = 1\nn_layers = 2\nbidirectional = True\ndropout = 0.2\nPAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\npatience=3","metadata":{"id":"ggTiORJ-8t0J","execution":{"iopub.status.busy":"2023-05-19T10:44:32.590404Z","iopub.execute_input":"2023-05-19T10:44:32.592799Z","iopub.status.idle":"2023-05-19T10:44:32.602034Z","shell.execute_reply.started":"2023-05-19T10:44:32.592768Z","shell.execute_reply":"2023-05-19T10:44:32.601414Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"model = RNNBaseline(\n    vocab_size=vocab_size,\n    embedding_dim=emb_dim,\n    hidden_dim=hidden_dim,\n    output_dim=output_dim,\n    n_layers=n_layers,\n    bidirectional=bidirectional,\n    dropout=dropout,\n    pad_idx=PAD_IDX\n)","metadata":{"id":"mIiM_ZBt9_91","execution":{"iopub.status.busy":"2023-05-19T10:44:32.605292Z","iopub.execute_input":"2023-05-19T10:44:32.607866Z","iopub.status.idle":"2023-05-19T10:44:32.827571Z","shell.execute_reply.started":"2023-05-19T10:44:32.607830Z","shell.execute_reply":"2023-05-19T10:44:32.826613Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model = model.to(device)","metadata":{"id":"mFeG88M--NbD","execution":{"iopub.status.busy":"2023-05-19T10:44:32.829110Z","iopub.execute_input":"2023-05-19T10:44:32.829541Z","iopub.status.idle":"2023-05-19T10:44:35.742228Z","shell.execute_reply.started":"2023-05-19T10:44:32.829502Z","shell.execute_reply":"2023-05-19T10:44:35.741281Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"opt = torch.optim.Adam(model.parameters())\nloss_func = nn.CrossEntropyLoss()\n\nmax_epochs = 10","metadata":{"id":"olAS-mVI-VfT","execution":{"iopub.status.busy":"2023-05-19T10:44:35.743740Z","iopub.execute_input":"2023-05-19T10:44:35.744158Z","iopub.status.idle":"2023-05-19T10:44:35.750686Z","shell.execute_reply.started":"2023-05-19T10:44:35.744117Z","shell.execute_reply":"2023-05-19T10:44:35.749662Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Обучите сетку! Используйте любые вам удобные инструменты, Catalyst, PyTorch Lightning или свои велосипеды.","metadata":{"id":"5jlfEAJu9akO"}},{"cell_type":"code","source":"import numpy as np\n\ndef train_model(model, epochs, opt, loss_func):\n    min_loss = np.inf\n\n    max_grad_norm=2\n    torch.cuda.empty_cache()\n\n    for epoch in range(1, epochs + 1):\n\n        train_loss = 0.0\n        model.train()\n        pbar = tqdm(enumerate(train_iter), total=len(train_iter), leave=False)\n        pbar.set_description(f\"Epoch {epoch}\")\n        for it, batch in pbar: \n            opt.zero_grad()\n\n            input_embeds = batch.text[0].to(device).permute(1, 0)\n            labels = batch.label.to(device).to(torch.int64)\n\n            prediction = model(input_embeds)\n\n            loss = loss_func(prediction, labels)\n\n            train_loss += loss\n\n            loss.backward()\n\n            if max_grad_norm is not None:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n\n            opt.step()\n\n        train_loss /= len(train_iter)\n        val_loss = 0.0\n        model.eval()\n        pbar = tqdm(enumerate(valid_iter), total=len(valid_iter), leave=False)\n        pbar.set_description(f\"Epoch {epoch}\")\n        for it, batch in pbar:\n            with torch.no_grad():\n\n                input_embeds = batch.text[0].to(device).permute(1, 0)\n                labels = batch.label.to(device).to(torch.int64)\n\n                prediction = model(input_embeds)\n\n                loss = loss_func(prediction, labels)\n                val_loss += loss\n\n        val_loss /= len(valid_iter)\n        \n        if val_loss < min_loss:\n            min_loss = val_loss\n            best_model = model.state_dict()\n\n        print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, train_loss, val_loss))\n        model.load_state_dict(best_model)","metadata":{"execution":{"iopub.status.busy":"2023-05-19T10:44:35.755546Z","iopub.execute_input":"2023-05-19T10:44:35.756509Z","iopub.status.idle":"2023-05-19T10:44:35.770015Z","shell.execute_reply.started":"2023-05-19T10:44:35.756476Z","shell.execute_reply":"2023-05-19T10:44:35.769073Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train_model(model, max_epochs, opt, loss_func)","metadata":{"id":"87dgw6ok9hR0","execution":{"iopub.status.busy":"2023-05-19T10:45:01.771877Z","iopub.execute_input":"2023-05-19T10:45:01.772330Z","iopub.status.idle":"2023-05-19T11:01:51.670959Z","shell.execute_reply.started":"2023-05-19T10:45:01.772296Z","shell.execute_reply":"2023-05-19T11:01:51.669551Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/274 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/118 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch: 1, Training Loss: 0.6931580305099487, Validation Loss: 0.6942285299301147\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/274 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/118 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch: 2, Training Loss: 0.6829893589019775, Validation Loss: 0.6904247999191284\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/274 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/118 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch: 3, Training Loss: 0.6729505062103271, Validation Loss: 0.6895972490310669\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/274 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/118 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch: 4, Training Loss: 0.6633146405220032, Validation Loss: 0.6871888041496277\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/274 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/118 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch: 5, Training Loss: 0.6549070477485657, Validation Loss: 0.6922739744186401\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/274 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/118 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch: 6, Training Loss: 0.6482081413269043, Validation Loss: 0.6818742752075195\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/274 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/118 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch: 7, Training Loss: 0.6411721110343933, Validation Loss: 0.6800276041030884\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/274 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/118 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch: 8, Training Loss: 0.6368064880371094, Validation Loss: 0.6801760196685791\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/274 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/118 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch: 9, Training Loss: 0.6286424398422241, Validation Loss: 0.6774827837944031\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/274 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/118 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch: 10, Training Loss: 0.61618971824646, Validation Loss: 0.6806517243385315\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Посчитайте f1-score вашего классификатора на тестовом датасете.\n\n**Ответ**:","metadata":{"id":"y4i-Go_ICT_U"}},{"cell_type":"code","source":"from sklearn.metrics import f1_score\n\ndef calculate_f1_score(model):\n    with torch.no_grad():\n        \n        Y_REAL = np.array([])\n        Y_PRED = np.array([])\n        \n        for batch in test_iter:\n            input_embeds = batch.text[0].to(device).permute(1, 0)\n            labels = batch.label.to(device).to(torch.int64)\n            prediction = model(input_embeds)\n            \n            y_real = labels.cpu().numpy()\n            y_pred = np.array([ 0 if i[0]>i[1] else 1 for i in prediction])\n            \n            Y_REAL = np.concatenate((Y_REAL, y_real), axis=0)\n            Y_PRED = np.concatenate((Y_PRED, y_pred), axis=0)\n            \n        \n            \n        return f1_score(Y_REAL, Y_PRED)\n    ","metadata":{"id":"gWkbCpNECflR","execution":{"iopub.status.busy":"2023-05-19T11:02:02.733799Z","iopub.execute_input":"2023-05-19T11:02:02.734169Z","iopub.status.idle":"2023-05-19T11:02:02.742781Z","shell.execute_reply.started":"2023-05-19T11:02:02.734134Z","shell.execute_reply":"2023-05-19T11:02:02.741654Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"calculate_f1_score(model)","metadata":{"execution":{"iopub.status.busy":"2023-05-19T11:02:04.076721Z","iopub.execute_input":"2023-05-19T11:02:04.077399Z","iopub.status.idle":"2023-05-19T11:02:35.015555Z","shell.execute_reply.started":"2023-05-19T11:02:04.077366Z","shell.execute_reply":"2023-05-19T11:02:35.014605Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"0.6184233283752465"},"metadata":{}}]},{"cell_type":"markdown","source":"Модель плохо обучилась","metadata":{}},{"cell_type":"markdown","source":"## CNN\n\n![](https://www.researchgate.net/publication/333752473/figure/fig1/AS:769346934673412@1560438011375/Standard-CNN-on-text-classification.png)\n\nДля классификации текстов также часто используют сверточные нейронные сети. Идея в том, что как правило сентимент содержат словосочетания из двух-трех слов, например \"очень хороший фильм\" или \"невероятная скука\". Проходясь сверткой по этим словам мы получим какой-то большой скор и выхватим его с помощью MaxPool. Далее идет обычная полносвязная сетка. Важный момент: свертки применяются не последовательно, а параллельно. Давайте попробуем!","metadata":{"id":"kzDqc3__JIMe"}},{"cell_type":"code","source":"TEXT = Field(sequential=True, lower=True, batch_first=True)  # batch_first тк мы используем conv  \nLABEL = LabelField(batch_first=True, dtype=torch.float)\n\ntrain, tst = datasets.IMDB.splits(TEXT, LABEL)\ntrn, vld = train.split(random_state=random.seed(SEED))\n\nTEXT.build_vocab(trn)\nLABEL.build_vocab(trn)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"id":"rU-76tNI-STt","execution":{"iopub.status.busy":"2023-05-19T19:22:46.975190Z","iopub.execute_input":"2023-05-19T19:22:46.975665Z","iopub.status.idle":"2023-05-19T19:22:54.975523Z","shell.execute_reply.started":"2023-05-19T19:22:46.975630Z","shell.execute_reply":"2023-05-19T19:22:54.974539Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_iter, val_iter, test_iter = BucketIterator.splits(\n        (trn, vld, tst),\n        batch_sizes=(128, 256, 256),\n        sort=False,\n        sort_key= lambda x: len(x.src),\n        sort_within_batch=False,\n        device=device,\n        repeat=False,\n)","metadata":{"id":"RQpS9KKUJQVH","execution":{"iopub.status.busy":"2023-05-19T19:22:54.978225Z","iopub.execute_input":"2023-05-19T19:22:54.978896Z","iopub.status.idle":"2023-05-19T19:22:54.984689Z","shell.execute_reply.started":"2023-05-19T19:22:54.978861Z","shell.execute_reply":"2023-05-19T19:22:54.983765Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Вы можете использовать Conv2d с `in_channels=1, kernel_size=(kernel_sizes[0], emb_dim))` или Conv1d c `in_channels=emb_dim, kernel_size=kernel_size[0]`. Но хорошенько подумайте над shape в обоих случаях.","metadata":{"id":"asgbwMePPNNl"}},{"cell_type":"code","source":"class CNN(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        emb_dim,\n        out_channels,\n        kernel_sizes,\n        dropout=0.5,\n    ):\n        super().__init__()\n        \n        self.embedding = nn.Embedding(vocab_size, emb_dim)\n        \n        self.conv_0 = nn.Conv1d(emb_dim, out_channels, kernel_size=kernel_sizes[0], padding=1, stride=1) # YOUR CODE GOES HERE\n        \n        self.conv_1 = nn.Conv1d(emb_dim, out_channels, kernel_size=kernel_sizes[1], padding=1, stride=1)  # YOUR CODE GOES HERE\n        \n        self.conv_2 = nn.Conv1d(emb_dim, out_channels, kernel_size=kernel_sizes[2], padding=1, stride=1)  # YOUR CODE GOES HERE\n        \n        self.fc = nn.Linear(len(kernel_sizes) * out_channels, 1)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n        \n    def forward(self, text):\n        \n        embedded = self.embedding(text)\n        \n#         print(\"embedded:\", embedded.shape)\n        \n        embedded = embedded.permute(0, 2, 1)  # may be reshape here\n        \n        conved_0 = F.relu(self.conv_0(embedded))  # may be reshape here\n        conved_1 = F.relu(self.conv_1(embedded))  # may be reshape here\n        conved_2 = F.relu(self.conv_2(embedded))  # may be reshape here\n        \n        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n        \n        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n            \n        return self.fc(cat)","metadata":{"id":"qPP_-0E-JYTQ","execution":{"iopub.status.busy":"2023-05-19T19:22:54.986199Z","iopub.execute_input":"2023-05-19T19:22:54.986715Z","iopub.status.idle":"2023-05-19T19:22:54.999802Z","shell.execute_reply.started":"2023-05-19T19:22:54.986683Z","shell.execute_reply":"2023-05-19T19:22:54.998785Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"kernel_sizes = [3, 4, 5]\nvocab_size = len(TEXT.vocab)\nout_channels=64\ndropout = 0.5\ndim = 300\n\nmodel = CNN(vocab_size=vocab_size, emb_dim=dim, out_channels=out_channels,\n            kernel_sizes=kernel_sizes, dropout=dropout)","metadata":{"id":"Y-U_2T5oKNed","execution":{"iopub.status.busy":"2023-05-19T11:02:50.263076Z","iopub.execute_input":"2023-05-19T11:02:50.263369Z","iopub.status.idle":"2023-05-19T11:02:50.800393Z","shell.execute_reply.started":"2023-05-19T11:02:50.263345Z","shell.execute_reply":"2023-05-19T11:02:50.799296Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"model.to(device)","metadata":{"id":"vC2ThnfNKPIR","execution":{"iopub.status.busy":"2023-05-19T11:02:50.802023Z","iopub.execute_input":"2023-05-19T11:02:50.802482Z","iopub.status.idle":"2023-05-19T11:02:50.877635Z","shell.execute_reply.started":"2023-05-19T11:02:50.802447Z","shell.execute_reply":"2023-05-19T11:02:50.876654Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"CNN(\n  (embedding): Embedding(202864, 300)\n  (conv_0): Conv1d(300, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n  (conv_1): Conv1d(300, 64, kernel_size=(4,), stride=(1,), padding=(1,))\n  (conv_2): Conv1d(300, 64, kernel_size=(5,), stride=(1,), padding=(1,))\n  (fc): Linear(in_features=192, out_features=1, bias=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"opt = torch.optim.Adam(model.parameters())\nloss_func = nn.BCEWithLogitsLoss()","metadata":{"id":"mExblVtPKRw4","execution":{"iopub.status.busy":"2023-05-19T19:28:43.774543Z","iopub.execute_input":"2023-05-19T19:28:43.774936Z","iopub.status.idle":"2023-05-19T19:28:43.783536Z","shell.execute_reply.started":"2023-05-19T19:28:43.774906Z","shell.execute_reply":"2023-05-19T19:28:43.782379Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"max_epochs = 30","metadata":{"id":"QVwSgwkEKTw5","execution":{"iopub.status.busy":"2023-05-19T11:02:50.888848Z","iopub.execute_input":"2023-05-19T11:02:50.889221Z","iopub.status.idle":"2023-05-19T11:02:50.896899Z","shell.execute_reply.started":"2023-05-19T11:02:50.889187Z","shell.execute_reply":"2023-05-19T11:02:50.895717Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"Обучите!","metadata":{"id":"VIQgLCELDoOA"}},{"cell_type":"code","source":"import numpy as np\n\nmin_loss = np.inf\npatience = 7\ncur_patience = 0\n\nfor epoch in range(1, max_epochs + 1):\n    train_loss = 0.0\n    model.train()\n    pbar = tqdm(enumerate(train_iter), total=len(train_iter), leave=False)\n    pbar.set_description(f\"Epoch {epoch}\")\n    for it, batch in pbar: \n        opt.zero_grad()\n\n        input_embeds = batch.text.to(device)\n        labels = batch.label.to(device)\n\n        prediction = model(input_embeds).squeeze(1)\n\n        loss = loss_func(prediction, labels)\n\n        train_loss += loss\n\n        loss.backward()\n\n        opt.step()\n\n    train_loss /= len(train_iter)\n    val_loss = 0.0\n    model.eval()\n    pbar = tqdm(enumerate(val_iter), total=len(val_iter), leave=False)\n    pbar.set_description(f\"Epoch {epoch}\")\n    for it, batch in pbar:\n        with torch.no_grad():\n            \n#             print(batch)\n            \n            input_embeds = batch.text.to(device)\n            labels = batch.label.to(device)\n\n            prediction = model(input_embeds).squeeze(1)\n\n            loss = loss_func(prediction, labels)\n            val_loss += loss\n    val_loss /= len(val_iter)\n    if val_loss < min_loss:\n        min_loss = val_loss\n        best_model = model.state_dict()\n    else:\n        cur_patience += 1\n        if cur_patience == patience:\n            cur_patience = 0\n            break\n    \n    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, train_loss, val_loss))\nmodel.load_state_dict(best_model)","metadata":{"id":"zQZbJ791KXHb","execution":{"iopub.status.busy":"2023-05-19T11:02:50.899145Z","iopub.execute_input":"2023-05-19T11:02:50.900661Z","iopub.status.idle":"2023-05-19T11:06:47.073042Z","shell.execute_reply.started":"2023-05-19T11:02:50.900625Z","shell.execute_reply":"2023-05-19T11:06:47.071659Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/137 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch: 1, Training Loss: 0.6454027891159058, Validation Loss: 0.5011653304100037\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/137 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch: 2, Training Loss: 0.5069141387939453, Validation Loss: 0.44733506441116333\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/137 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch: 3, Training Loss: 0.43202024698257446, Validation Loss: 0.3995462656021118\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/137 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch: 4, Training Loss: 0.374947190284729, Validation Loss: 0.3701707422733307\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/137 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch: 5, Training Loss: 0.31983616948127747, Validation Loss: 0.3458961248397827\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/137 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch: 6, Training Loss: 0.26218491792678833, Validation Loss: 0.3304813802242279\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/137 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch: 7, Training Loss: 0.2013900876045227, Validation Loss: 0.32898902893066406\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/137 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch: 8, Training Loss: 0.1442285031080246, Validation Loss: 0.34297195076942444\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/137 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch: 9, Training Loss: 0.10854243487119675, Validation Loss: 0.3503020703792572\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/137 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch: 10, Training Loss: 0.06917991489171982, Validation Loss: 0.3809279501438141\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/137 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch: 11, Training Loss: 0.0497843362390995, Validation Loss: 0.40068647265434265\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/137 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch: 12, Training Loss: 0.03308293968439102, Validation Loss: 0.4216243028640747\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/137 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch: 13, Training Loss: 0.02139042317867279, Validation Loss: 0.46755799651145935\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/137 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"markdown","source":"Посчитайте f1-score вашего классификатора.\n\n**Ответ**:","metadata":{"id":"1UVCacK0EhPR"}},{"cell_type":"code","source":"from sklearn.metrics import f1_score\n\nwith torch.no_grad():\n        Y_REAL = np.array([])\n        Y_PRED = np.array([])\n        \n        for batch in test_iter:\n            input_embeds = batch.text.to(device)\n            labels = batch.label.to(device)\n            \n                \n            prediction = model(input_embeds).squeeze(1)\n            \n            \n            y_real = labels.cpu().numpy()\n            y_pred = np.array([1 if i > 0 else 0 for i in prediction.cpu()])\n            \n            Y_REAL = np.concatenate((Y_REAL, y_real), axis=0)\n            Y_PRED = np.concatenate((Y_PRED, y_pred), axis=0)\n            \n        print(f1_score(Y_REAL, Y_PRED))","metadata":{"execution":{"iopub.status.busy":"2023-05-19T11:06:47.076407Z","iopub.execute_input":"2023-05-19T11:06:47.077046Z","iopub.status.idle":"2023-05-19T11:06:59.074755Z","shell.execute_reply.started":"2023-05-19T11:06:47.077004Z","shell.execute_reply":"2023-05-19T11:06:59.073748Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"0.8513523986669767\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Неплохой скор","metadata":{}},{"cell_type":"markdown","source":"## Интерпретируемость\n\nПосмотрим, куда смотрит наша модель. Достаточно запустить код ниже.","metadata":{"id":"7VspGMN0ESiS"}},{"cell_type":"code","source":"!pip install -q captum","metadata":{"id":"ye2SvjXrPgJh","execution":{"iopub.status.busy":"2023-05-19T19:32:41.159261Z","iopub.execute_input":"2023-05-19T19:32:41.159642Z","iopub.status.idle":"2023-05-19T19:32:53.665823Z","shell.execute_reply.started":"2023-05-19T19:32:41.159612Z","shell.execute_reply":"2023-05-19T19:32:53.664614Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from captum.attr import LayerIntegratedGradients, TokenReferenceBase, visualization\n\nPAD_IND = TEXT.vocab.stoi['pad']\n\ntoken_reference = TokenReferenceBase(reference_token_idx=PAD_IND)\nlig = LayerIntegratedGradients(model, model.embedding)","metadata":{"id":"6e5XPKSZO6DY","execution":{"iopub.status.busy":"2023-05-19T19:32:53.668937Z","iopub.execute_input":"2023-05-19T19:32:53.669346Z","iopub.status.idle":"2023-05-19T19:32:53.754437Z","shell.execute_reply.started":"2023-05-19T19:32:53.669305Z","shell.execute_reply":"2023-05-19T19:32:53.753424Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def forward_with_softmax(inp):\n    logits = model(inp)\n    return torch.softmax(logits, 0)[0][1]\n\ndef forward_with_sigmoid(input):\n    return torch.sigmoid(model(input))\n\n\n# accumalate couple samples in this array for visualization purposes\nvis_data_records_ig = []\n\ndef interpret_sentence(model, sentence, min_len = 7, label = 0):\n    model.eval()\n    text = [tok for tok in TEXT.tokenize(sentence)]\n    if len(text) < min_len:\n        text += ['pad'] * (min_len - len(text))\n    indexed = [TEXT.vocab.stoi[t] for t in text]\n\n    model.zero_grad()\n\n    input_indices = torch.tensor(indexed, device=device)\n    input_indices = input_indices.unsqueeze(0)\n    \n    # input_indices dim: [sequence_length]\n    seq_length = min_len\n\n    # predict\n    pred = forward_with_sigmoid(input_indices).item()\n    pred_ind = round(pred)\n\n    # generate reference indices for each sample\n    reference_indices = token_reference.generate_reference(seq_length, device=device).unsqueeze(0)\n\n    # compute attributions and approximation delta using layer integrated gradients\n    attributions_ig, delta = lig.attribute(input_indices, reference_indices, \\\n                                           n_steps=5000, return_convergence_delta=True)\n\n    print('pred: ', LABEL.vocab.itos[pred_ind], '(', '%.2f'%pred, ')', ', delta: ', abs(delta))\n\n    add_attributions_to_visualizer(attributions_ig, text, pred, pred_ind, label, delta, vis_data_records_ig)\n    \ndef add_attributions_to_visualizer(attributions, text, pred, pred_ind, label, delta, vis_data_records):\n    attributions = attributions.sum(dim=2).squeeze(0)\n    attributions = attributions / torch.norm(attributions)\n    attributions = attributions.cpu().detach().numpy()\n\n    # storing couple samples in an array for visualization purposes\n    vis_data_records.append(visualization.VisualizationDataRecord(\n                            attributions,\n                            pred,\n                            LABEL.vocab.itos[pred_ind],\n                            LABEL.vocab.itos[label],\n                            LABEL.vocab.itos[1],\n                            attributions.sum(),       \n                            text,\n                            delta))","metadata":{"id":"DvqWhd-fPe9e","execution":{"iopub.status.busy":"2023-05-19T19:32:53.756239Z","iopub.execute_input":"2023-05-19T19:32:53.756623Z","iopub.status.idle":"2023-05-19T19:32:53.772014Z","shell.execute_reply.started":"2023-05-19T19:32:53.756580Z","shell.execute_reply":"2023-05-19T19:32:53.770899Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"interpret_sentence(model, 'It was a fantastic performance !', label=1)\ninterpret_sentence(model, 'Best film ever', label=1)\ninterpret_sentence(model, 'Such a great show!', label=1)\ninterpret_sentence(model, 'It was a horrible movie', label=0)\ninterpret_sentence(model, 'I\\'ve never watched something as bad', label=0)\ninterpret_sentence(model, 'It is a disgusting movie!', label=0)","metadata":{"id":"VtYy633vS8Me","execution":{"iopub.status.busy":"2023-05-19T11:07:23.225261Z","iopub.execute_input":"2023-05-19T11:07:23.225604Z","iopub.status.idle":"2023-05-19T11:07:43.057359Z","shell.execute_reply.started":"2023-05-19T11:07:23.225571Z","shell.execute_reply":"2023-05-19T11:07:43.055656Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"pred:  pos ( 0.97 ) , delta:  tensor([0.0002], device='cuda:0', dtype=torch.float64)\npred:  neg ( 0.00 ) , delta:  tensor([9.1591e-05], device='cuda:0', dtype=torch.float64)\npred:  neg ( 0.03 ) , delta:  tensor([5.0434e-05], device='cuda:0', dtype=torch.float64)\npred:  neg ( 0.00 ) , delta:  tensor([6.1890e-05], device='cuda:0', dtype=torch.float64)\npred:  neg ( 0.02 ) , delta:  tensor([4.1112e-05], device='cuda:0', dtype=torch.float64)\npred:  neg ( 0.00 ) , delta:  tensor([0.0002], device='cuda:0', dtype=torch.float64)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Попробуйте добавить свои примеры!","metadata":{"id":"aqIRSCWlRTOe"}},{"cell_type":"code","source":"print('Visualize attributions based on Integrated Gradients')\nvisualization.visualize_text(vis_data_records_ig)","metadata":{"id":"4URAkcWXTGBi","execution":{"iopub.status.busy":"2023-05-19T11:07:43.058872Z","iopub.execute_input":"2023-05-19T11:07:43.059232Z","iopub.status.idle":"2023-05-19T11:07:43.070904Z","shell.execute_reply.started":"2023-05-19T11:07:43.059198Z","shell.execute_reply":"2023-05-19T11:07:43.069803Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Visualize attributions based on Integrated Gradients\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>pos (0.97)</b></text></td><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>1.86</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> It                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 66%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> fantastic                    </font></mark><mark style=\"background-color: hsl(120, 75%, 71%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> performance                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> !                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>neg (0.00)</b></text></td><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>1.55</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Best                    </font></mark><mark style=\"background-color: hsl(120, 75%, 68%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> film                    </font></mark><mark style=\"background-color: hsl(120, 75%, 64%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ever                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>neg (0.03)</b></text></td><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>0.79</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Such                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 52%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> great                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> show!                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>neg</b></text></td><td><text style=\"padding-right:2em\"><b>neg (0.00)</b></text></td><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>-0.12</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> It                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 68%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> horrible                    </font></mark><mark style=\"background-color: hsl(120, 75%, 73%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>neg</b></text></td><td><text style=\"padding-right:2em\"><b>neg (0.02)</b></text></td><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>0.59</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> I've                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> never                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> watched                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> something                    </font></mark><mark style=\"background-color: hsl(120, 75%, 62%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(0, 75%, 77%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>neg</b></text></td><td><text style=\"padding-right:2em\"><b>neg (0.00)</b></text></td><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>0.48</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> It                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 78%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> disgusting                    </font></mark><mark style=\"background-color: hsl(120, 75%, 61%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie!                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark></td><tr></table>"},"metadata":{}},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>pos (0.97)</b></text></td><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>1.86</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> It                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 66%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> fantastic                    </font></mark><mark style=\"background-color: hsl(120, 75%, 71%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> performance                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> !                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>neg (0.00)</b></text></td><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>1.55</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Best                    </font></mark><mark style=\"background-color: hsl(120, 75%, 68%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> film                    </font></mark><mark style=\"background-color: hsl(120, 75%, 64%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ever                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>neg (0.03)</b></text></td><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>0.79</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Such                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 52%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> great                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> show!                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>neg</b></text></td><td><text style=\"padding-right:2em\"><b>neg (0.00)</b></text></td><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>-0.12</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> It                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 68%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> horrible                    </font></mark><mark style=\"background-color: hsl(120, 75%, 73%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>neg</b></text></td><td><text style=\"padding-right:2em\"><b>neg (0.02)</b></text></td><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>0.59</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> I've                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> never                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> watched                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> something                    </font></mark><mark style=\"background-color: hsl(120, 75%, 62%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(0, 75%, 77%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>neg</b></text></td><td><text style=\"padding-right:2em\"><b>neg (0.00)</b></text></td><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>0.48</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> It                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 78%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> disgusting                    </font></mark><mark style=\"background-color: hsl(120, 75%, 61%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie!                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark></td><tr></table>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Эмбеддинги слов\n\nВы ведь не забыли, как мы можем применить знания о word2vec и GloVe. Давайте попробуем!","metadata":{"id":"SvEHEaurElu8"}},{"cell_type":"code","source":"TEXT.build_vocab(trn, vectors= GloVe(name='6B', dim=300) )","metadata":{"id":"iW46gGLNuo0q","execution":{"iopub.status.busy":"2023-05-19T19:23:04.472051Z","iopub.execute_input":"2023-05-19T19:23:04.472437Z","iopub.status.idle":"2023-05-19T19:27:09.482888Z","shell.execute_reply.started":"2023-05-19T19:23:04.472406Z","shell.execute_reply":"2023-05-19T19:27:09.481868Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":".vector_cache/glove.6B.zip: 862MB [02:41, 5.35MB/s]                               \n100%|█████████▉| 399999/400000 [01:03<00:00, 6296.30it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"LABEL.build_vocab(trn)\n\nword_embeddings = TEXT.vocab.vectors\n\nkernel_sizes = [3, 4, 5]\nvocab_size = len(TEXT.vocab)\ndropout = 0.5\ndim = 300","metadata":{"execution":{"iopub.status.busy":"2023-05-19T19:27:09.484877Z","iopub.execute_input":"2023-05-19T19:27:09.485252Z","iopub.status.idle":"2023-05-19T19:27:09.523101Z","shell.execute_reply.started":"2023-05-19T19:27:09.485219Z","shell.execute_reply":"2023-05-19T19:27:09.522217Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train, tst = datasets.IMDB.splits(TEXT, LABEL)\ntrn, vld = train.split(random_state=random.seed(SEED))\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ntrain_iter, val_iter, test_iter = BucketIterator.splits(\n        (trn, vld, tst),\n        batch_sizes=(128, 256, 256),\n        sort=False,\n        sort_key= lambda x: len(x.src),\n        sort_within_batch=False,\n        device=device,\n        repeat=False,\n)","metadata":{"id":"MZ4YwLlcltm3","execution":{"iopub.status.busy":"2023-05-19T19:27:09.524509Z","iopub.execute_input":"2023-05-19T19:27:09.524862Z","iopub.status.idle":"2023-05-19T19:27:17.333822Z","shell.execute_reply.started":"2023-05-19T19:27:09.524827Z","shell.execute_reply":"2023-05-19T19:27:17.332772Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"model = CNN(vocab_size=vocab_size, emb_dim=dim, out_channels=64,\n            kernel_sizes=kernel_sizes, dropout=dropout)","metadata":{"execution":{"iopub.status.busy":"2023-05-19T19:27:17.336498Z","iopub.execute_input":"2023-05-19T19:27:17.337109Z","iopub.status.idle":"2023-05-19T19:27:17.853734Z","shell.execute_reply.started":"2023-05-19T19:27:17.337073Z","shell.execute_reply":"2023-05-19T19:27:17.852785Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"\n\nword_embeddings = TEXT.vocab.vectors\n\nprev_shape = model.embedding.weight.shape\n\nmodel.embedding.weight = nn.Parameter(TEXT.vocab.vectors)\n\nassert prev_shape == model.embedding.weight.shape\nmodel.to(device)\n\nopt = torch.optim.Adam(model.parameters())","metadata":{"id":"2l5pDvZgl7Fp","execution":{"iopub.status.busy":"2023-05-19T19:27:17.854996Z","iopub.execute_input":"2023-05-19T19:27:17.855330Z","iopub.status.idle":"2023-05-19T19:27:20.711830Z","shell.execute_reply.started":"2023-05-19T19:27:17.855300Z","shell.execute_reply":"2023-05-19T19:27:20.710871Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Вы знаете, что делать.","metadata":{"id":"IwiQjcuiFGTC"}},{"cell_type":"code","source":"import numpy as np\n\nmin_loss = np.inf\npatience = 7\ncur_patience = 0\n\nmax_epochs = 30\n\nfor epoch in range(1, max_epochs + 1):\n    train_loss = 0.0\n    model.train()\n    pbar = tqdm(enumerate(train_iter), total=len(train_iter), leave=False)\n    pbar.set_description(f\"Epoch {epoch}\")\n    for it, batch in pbar: \n        opt.zero_grad()\n\n        input_embeds = batch.text.to(device)\n        labels = batch.label.to(device)\n\n        prediction = model(input_embeds).squeeze(1)\n\n        loss = loss_func(prediction, labels)\n\n        train_loss += loss\n\n        loss.backward()\n\n        opt.step()\n\n    train_loss /= len(train_iter)\n    val_loss = 0.0\n    model.eval()\n    pbar = tqdm(enumerate(val_iter), total=len(val_iter), leave=False)\n    pbar.set_description(f\"Epoch {epoch}\")\n    for it, batch in pbar:\n        with torch.no_grad():\n            \n#             print(batch)\n            \n            input_embeds = batch.text.to(device)\n            labels = batch.label.to(device)\n\n            prediction = model(input_embeds).squeeze(1)\n\n            loss = loss_func(prediction, labels)\n            val_loss += loss\n    val_loss /= len(val_iter)\n    if val_loss < min_loss:\n        min_loss = val_loss\n        best_model = model.state_dict()\n    else:\n        cur_patience += 1\n        if cur_patience == patience:\n            cur_patience = 0\n            break\n    \n    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, train_loss, val_loss))\nmodel.load_state_dict(best_model)","metadata":{"id":"fNqcFHT8cT0b","execution":{"iopub.status.busy":"2023-05-19T19:28:52.484435Z","iopub.execute_input":"2023-05-19T19:28:52.484931Z","iopub.status.idle":"2023-05-19T19:31:41.290670Z","shell.execute_reply.started":"2023-05-19T19:28:52.484892Z","shell.execute_reply":"2023-05-19T19:31:41.289515Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/137 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch: 1, Training Loss: 0.5045396685600281, Validation Loss: 0.359258234500885\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/137 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch: 2, Training Loss: 0.3095386326313019, Validation Loss: 0.3112695515155792\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/137 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch: 3, Training Loss: 0.18196246027946472, Validation Loss: 0.3002704679965973\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/137 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch: 4, Training Loss: 0.07930988818407059, Validation Loss: 0.32665547728538513\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/137 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch: 5, Training Loss: 0.029300609603524208, Validation Loss: 0.3573606610298157\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/137 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch: 6, Training Loss: 0.013634847477078438, Validation Loss: 0.3892107605934143\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/137 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch: 7, Training Loss: 0.0070083728060126305, Validation Loss: 0.411551833152771\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/137 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch: 8, Training Loss: 0.004550900775939226, Validation Loss: 0.43148455023765564\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/137 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch: 9, Training Loss: 0.0030237233731895685, Validation Loss: 0.44652673602104187\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/137 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"markdown","source":"Посчитайте f1-score вашего классификатора.\n\n**Ответ**:","metadata":{"id":"R2IdEWJQKESg"}},{"cell_type":"code","source":"from sklearn.metrics import f1_score\n\nwith torch.no_grad():\n        Y_REAL = np.array([])\n        Y_PRED = np.array([])\n        \n        for batch in test_iter:\n            input_embeds = batch.text.to(device)\n            labels = batch.label.to(device)\n            \n                \n            prediction = model(input_embeds).squeeze(1)\n            \n            \n            y_real = labels.cpu().numpy()\n            y_pred = np.array([1 if i > 0 else 0 for i in prediction.cpu()])\n            \n            Y_REAL = np.concatenate((Y_REAL, y_real), axis=0)\n            Y_PRED = np.concatenate((Y_PRED, y_pred), axis=0)\n            \n        print(f1_score(Y_REAL, Y_PRED))","metadata":{"id":"kizk028eRF0R","execution":{"iopub.status.busy":"2023-05-19T19:32:12.156785Z","iopub.execute_input":"2023-05-19T19:32:12.157174Z","iopub.status.idle":"2023-05-19T19:32:24.652521Z","shell.execute_reply.started":"2023-05-19T19:32:12.157143Z","shell.execute_reply":"2023-05-19T19:32:24.651455Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"0.8578134945894335\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Примерно то же качество и получилось","metadata":{}},{"cell_type":"markdown","source":"Проверим насколько все хорошо!","metadata":{"id":"4sl7h_wIRGPD"}},{"cell_type":"code","source":"PAD_IND = TEXT.vocab.stoi['pad']\n\ntoken_reference = TokenReferenceBase(reference_token_idx=PAD_IND)\nlig = LayerIntegratedGradients(model, model.embedding)\nvis_data_records_ig = []\n\ninterpret_sentence(model, 'It was a fantastic performance !', label=1)\ninterpret_sentence(model, 'Best film ever', label=1)\ninterpret_sentence(model, 'Such a great show!', label=1)\ninterpret_sentence(model, 'It was a horrible movie', label=0)\ninterpret_sentence(model, 'I\\'ve never watched something as bad', label=0)\ninterpret_sentence(model, 'It is a disgusting movie!', label=0)","metadata":{"id":"iPCm55FLir3e","execution":{"iopub.status.busy":"2023-05-19T19:32:53.774463Z","iopub.execute_input":"2023-05-19T19:32:53.775147Z","iopub.status.idle":"2023-05-19T19:33:14.791826Z","shell.execute_reply.started":"2023-05-19T19:32:53.775095Z","shell.execute_reply":"2023-05-19T19:33:14.789733Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"pred:  pos ( 0.78 ) , delta:  tensor([0.0001], device='cuda:0', dtype=torch.float64)\npred:  neg ( 0.00 ) , delta:  tensor([6.0350e-05], device='cuda:0', dtype=torch.float64)\npred:  neg ( 0.00 ) , delta:  tensor([4.0603e-05], device='cuda:0', dtype=torch.float64)\npred:  neg ( 0.00 ) , delta:  tensor([0.0001], device='cuda:0', dtype=torch.float64)\npred:  neg ( 0.01 ) , delta:  tensor([2.2441e-05], device='cuda:0', dtype=torch.float64)\npred:  neg ( 0.00 ) , delta:  tensor([0.0002], device='cuda:0', dtype=torch.float64)\n","output_type":"stream"}]},{"cell_type":"code","source":"print('Visualize attributions based on Integrated Gradients')\nvisualization.visualize_text(vis_data_records_ig)","metadata":{"id":"NMDazB3AlFWA","execution":{"iopub.status.busy":"2023-05-19T19:33:14.793601Z","iopub.execute_input":"2023-05-19T19:33:14.794438Z","iopub.status.idle":"2023-05-19T19:33:14.805476Z","shell.execute_reply.started":"2023-05-19T19:33:14.794393Z","shell.execute_reply":"2023-05-19T19:33:14.804485Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Visualize attributions based on Integrated Gradients\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>pos (0.78)</b></text></td><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>2.00</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> It                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 67%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> fantastic                    </font></mark><mark style=\"background-color: hsl(120, 75%, 70%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> performance                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> !                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>neg (0.00)</b></text></td><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>1.48</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Best                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> film                    </font></mark><mark style=\"background-color: hsl(120, 75%, 55%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ever                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>neg (0.00)</b></text></td><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>1.64</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Such                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 69%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> great                    </font></mark><mark style=\"background-color: hsl(120, 75%, 62%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> show!                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>neg</b></text></td><td><text style=\"padding-right:2em\"><b>neg (0.00)</b></text></td><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>1.41</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> It                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> horrible                    </font></mark><mark style=\"background-color: hsl(120, 75%, 56%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>neg</b></text></td><td><text style=\"padding-right:2em\"><b>neg (0.01)</b></text></td><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>1.97</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> I've                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> never                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> watched                    </font></mark><mark style=\"background-color: hsl(120, 75%, 67%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> something                    </font></mark><mark style=\"background-color: hsl(120, 75%, 70%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>neg</b></text></td><td><text style=\"padding-right:2em\"><b>neg (0.00)</b></text></td><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>1.42</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> It                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> disgusting                    </font></mark><mark style=\"background-color: hsl(120, 75%, 55%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie!                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark></td><tr></table>"},"metadata":{}},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>pos (0.78)</b></text></td><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>2.00</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> It                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 67%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> fantastic                    </font></mark><mark style=\"background-color: hsl(120, 75%, 70%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> performance                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> !                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>neg (0.00)</b></text></td><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>1.48</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Best                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> film                    </font></mark><mark style=\"background-color: hsl(120, 75%, 55%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ever                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>neg (0.00)</b></text></td><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>1.64</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Such                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 69%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> great                    </font></mark><mark style=\"background-color: hsl(120, 75%, 62%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> show!                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>neg</b></text></td><td><text style=\"padding-right:2em\"><b>neg (0.00)</b></text></td><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>1.41</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> It                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> horrible                    </font></mark><mark style=\"background-color: hsl(120, 75%, 56%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>neg</b></text></td><td><text style=\"padding-right:2em\"><b>neg (0.01)</b></text></td><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>1.97</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> I've                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> never                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> watched                    </font></mark><mark style=\"background-color: hsl(120, 75%, 67%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> something                    </font></mark><mark style=\"background-color: hsl(120, 75%, 70%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>neg</b></text></td><td><text style=\"padding-right:2em\"><b>neg (0.00)</b></text></td><td><text style=\"padding-right:2em\"><b>pos</b></text></td><td><text style=\"padding-right:2em\"><b>1.42</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> It                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> disgusting                    </font></mark><mark style=\"background-color: hsl(120, 75%, 55%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie!                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pad                    </font></mark></td><tr></table>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}